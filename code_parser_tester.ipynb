{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50884d42",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6687368",
   "metadata": {},
   "outputs": [],
   "source": [
    "Function_Path = \"Inventory.views_pack.terminal.saudi_tsr_output_checker\" # Inventory.views_pack.terminal.process_exe_data\n",
    "BASE_PATH = r\"C:\\Users\\JonathanChackoPattas\\OneDrive - Maritime Support Solutions\\Desktop\\MSS-Automation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e7a7a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import ast\n",
    "import os\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "from typing import Optional, Dict, Tuple, List, Union, Iterable, Set, Any\n",
    "# from google.adk.tools.tool_context import ToolContext\n",
    "\n",
    "FuncNode = Union[ast.FunctionDef, ast.AsyncFunctionDef]\n",
    "\n",
    "# -----------------------------\n",
    "# project indexing\n",
    "# -----------------------------\n",
    "\n",
    "_EXCLUDE_DIRS = {\n",
    "    \".git\", \"__pycache__\", \".mypy_cache\", \".pytest_cache\", \".ruff_cache\",\n",
    "    \"build\", \"dist\", \"site-packages\", \"venv\", \".venv\", \"env\", \".env\",\n",
    "    \".idea\", \".vscode\", \"node_modules\", \".tox\", \".eggs\",\n",
    "    \"venv-windows\", \"venv-linux\",\n",
    "}\n",
    "\n",
    "def create_file_path(base_path, function_path):\n",
    "    function_parts = function_path.split(\".\")\n",
    "    function_name = function_parts[-1]  # last part is the function\n",
    "    module_parts = function_parts[:-1]  # everything before is the module path\n",
    "    for part in module_parts:\n",
    "        base_path = os.path.join(base_path, part)\n",
    "    return base_path + \".py\", function_name\n",
    "\n",
    "def _iter_py_files(root: Path) -> Iterable[Path]:\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        # prune excluded dirs in-place for speed\n",
    "        dirnames[:] = [d for d in dirnames if d not in _EXCLUDE_DIRS]\n",
    "        for f in filenames:\n",
    "            if f.endswith(\".py\"):\n",
    "                yield Path(dirpath) / f\n",
    "\n",
    "def _to_module_qualname(base_path: Path, file_path: Path) -> str:\n",
    "    rel = file_path.relative_to(base_path)\n",
    "    if rel.name == \"__init__.py\":\n",
    "        rel = rel.parent\n",
    "    else:\n",
    "        rel = rel.with_suffix(\"\")\n",
    "    return \".\".join(rel.parts)\n",
    "\n",
    "def _to_function_path(base_path: Path, file_path: Path, func_name: str) -> str:\n",
    "    mod = _to_module_qualname(base_path, file_path)\n",
    "    return f\"{mod}.{func_name}\" if mod else func_name\n",
    "\n",
    "def _gather_defs(module: ast.Module) -> Tuple[Dict[str, FuncNode], Dict[str, Dict[str, FuncNode]]]:\n",
    "    \"\"\"Return (top_level_funcs, class_methods[class_name][func_name]).\"\"\"\n",
    "    top_level_funcs: Dict[str, FuncNode] = {}\n",
    "    class_methods: Dict[str, Dict[str, FuncNode]] = {}\n",
    "\n",
    "    for node in module.body:\n",
    "        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "            top_level_funcs[node.name] = node\n",
    "        elif isinstance(node, ast.ClassDef):\n",
    "            methods: Dict[str, FuncNode] = {}\n",
    "            for b in node.body:\n",
    "                if isinstance(b, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                    methods[b.name] = b\n",
    "            class_methods[node.name] = methods\n",
    "    return top_level_funcs, class_methods\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def _index_project_functions(base_path_str: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        by_name: Dict[str, List[tuple[Path, str /*module*/, FuncNode]]]\n",
    "        by_mod_func: Dict[str /*module.func*/, tuple[Path, FuncNode]]\n",
    "    \"\"\"\n",
    "    base_path = Path(base_path_str).resolve()\n",
    "    by_name: Dict[str, List[Tuple[Path, str, FuncNode]]] = {}\n",
    "    by_mod_func: Dict[str, Tuple[Path, FuncNode]] = {}\n",
    "\n",
    "    for py in _iter_py_files(base_path):\n",
    "        try:\n",
    "            src = py.read_text(encoding=\"utf-8\")\n",
    "            mod = ast.parse(src)\n",
    "        except Exception:\n",
    "            continue  # skip unreadable / syntactically invalid files\n",
    "\n",
    "        top_funcs, _ = _gather_defs(mod)\n",
    "        module_name = _to_module_qualname(base_path, py)\n",
    "        for name, node in top_funcs.items():\n",
    "            by_name.setdefault(name, []).append((py, module_name, node))\n",
    "            by_mod_func[f\"{module_name}.{name}\"] = (py, node)\n",
    "\n",
    "    return by_name, by_mod_func\n",
    "\n",
    "# -----------------------------\n",
    "# source slicing & calls\n",
    "# -----------------------------\n",
    "\n",
    "def _slice_with_decorators(src_lines: List[str], fn: FuncNode) -> Tuple[str, int, int]:\n",
    "    \"\"\"Return (code, start_line, end_line), 1-based line numbers inclusive.\"\"\"\n",
    "    start = fn.lineno\n",
    "    if getattr(fn, \"decorator_list\", None):\n",
    "        start = min(getattr(dec, \"lineno\", start) for dec in fn.decorator_list) or start\n",
    "    end = getattr(fn, \"end_lineno\", None)\n",
    "    if end is None:\n",
    "        full_src = \"\".join(src_lines)\n",
    "        seg = ast.get_source_segment(full_src, fn)\n",
    "        if seg is None:\n",
    "            raise RuntimeError(\"Unable to determine function end; please use Python 3.8+.\")\n",
    "        end = start + seg.count(\"\\n\")\n",
    "        return seg, start, end\n",
    "    return \"\\n\".join(src_lines[start - 1 : end]), start, end\n",
    "\n",
    "def _get_attr_chain(node: ast.AST) -> Tuple[Optional[str], List[str]]:\n",
    "    \"\"\"\n",
    "    For something like pkg.sub.mod.helper, return (\"pkg\", [\"sub\", \"mod\", \"helper\"]).\n",
    "    If not an attribute chain rooted at Name, return (None, []).\n",
    "    \"\"\"\n",
    "    chain: List[str] = []\n",
    "    cur = node\n",
    "    root_name = None\n",
    "    while isinstance(cur, ast.Attribute):\n",
    "        chain.append(cur.attr)\n",
    "        cur = cur.value\n",
    "    if isinstance(cur, ast.Name):\n",
    "        root_name = cur.id\n",
    "        chain.reverse()\n",
    "        return root_name, chain\n",
    "    return None, []\n",
    "\n",
    "def _collect_calls_and_locals(fn: FuncNode) -> tuple[set[str], list[tuple[str, list[str]]], set[str]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      bare: names of bare calls like {'helper', 'slugify'}\n",
    "      attrs: qualified calls like [('utils', ['slugify']), ('pkg', ['sub', 'do'])]\n",
    "      bound_locals: names bound in the function (params, assignments, etc.)\n",
    "    \"\"\"\n",
    "    bare: Set[str] = set()\n",
    "    attrs: List[Tuple[str, List[str]]] = []\n",
    "    bound_locals: Set[str] = set()\n",
    "\n",
    "    # params\n",
    "    args = fn.args\n",
    "    for a in getattr(args, \"posonlyargs\", []): bound_locals.add(a.arg)\n",
    "    for a in args.args: bound_locals.add(a.arg)\n",
    "    if args.vararg: bound_locals.add(args.vararg.arg)\n",
    "    for a in args.kwonlyargs: bound_locals.add(a.arg)\n",
    "    if args.kwarg: bound_locals.add(args.kwarg.arg)\n",
    "\n",
    "    def add_targets(t):\n",
    "        if isinstance(t, ast.Name):\n",
    "            bound_locals.add(t.id)\n",
    "        elif isinstance(t, (ast.Tuple, ast.List)):\n",
    "            for elt in t.elts:\n",
    "                add_targets(elt)\n",
    "\n",
    "    for n in ast.walk(fn):\n",
    "        if isinstance(n, ast.Call):\n",
    "            if isinstance(n.func, ast.Name):\n",
    "                bare.add(n.func.id)\n",
    "            else:\n",
    "                root, chain = _get_attr_chain(n.func)\n",
    "                if root and chain:\n",
    "                    attrs.append((root, chain))\n",
    "        elif isinstance(n, ast.Assign):\n",
    "            for t in n.targets: add_targets(t)\n",
    "        elif isinstance(n, ast.AnnAssign) and n.target:\n",
    "            add_targets(n.target)\n",
    "        elif isinstance(n, ast.AugAssign):\n",
    "            add_targets(n.target)\n",
    "        elif isinstance(n, ast.For):\n",
    "            add_targets(n.target)\n",
    "        elif isinstance(n, ast.With):\n",
    "            for item in n.items:\n",
    "                if item.optional_vars: add_targets(item.optional_vars)\n",
    "        elif isinstance(n, ast.comprehension):\n",
    "            add_targets(n.target)\n",
    "        elif isinstance(n, ast.ExceptHandler) and n.name:\n",
    "            bound_locals.add(n.name)\n",
    "\n",
    "    return bare, attrs, bound_locals\n",
    "\n",
    "# -----------------------------\n",
    "# import resolution\n",
    "# -----------------------------\n",
    "\n",
    "def _resolve_relative_module(this_module: str, level: int, module: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Resolve relative 'from ... import ...' to absolute dotted module.\n",
    "    this_module: e.g., 'Inventory.views_pack.terminal'\n",
    "    level: 1 => from . import x  (parent)\n",
    "    level: 2 => from ..pkg import y\n",
    "    \"\"\"\n",
    "    pkg_parts = this_module.split(\".\")[:-1]  # package of the file\n",
    "    if level > len(pkg_parts) + 1:\n",
    "        return None\n",
    "    base = pkg_parts[: len(pkg_parts) - (level - 1)]\n",
    "    if module:\n",
    "        base += module.split(\".\")\n",
    "    return \".\".join(p for p in base if p)\n",
    "\n",
    "def _parse_import_maps(mod: ast.Module, this_module: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      import_aliases: dict of local name -> absolute module dotted path\n",
    "         e.g., {'utils': 'Inventory.utils', 'mod': 'Inventory.x.y'}\n",
    "      from_names: dict of local imported symbol -> absolute module or module.symbol\n",
    "         e.g., {'slugify': 'Inventory.utils.slugify', 'utils': 'Inventory.utils'}\n",
    "    \"\"\"\n",
    "    import_aliases: Dict[str, str] = {}\n",
    "    from_names: Dict[str, str] = {}\n",
    "\n",
    "    for n in mod.body:\n",
    "        if isinstance(n, ast.Import):\n",
    "            for a in n.names:\n",
    "                full = a.name  # 'pkg' or 'pkg.sub.mod'\n",
    "                local = a.asname if a.asname else full.split(\".\")[0]\n",
    "                import_aliases[local] = full\n",
    "        elif isinstance(n, ast.ImportFrom):\n",
    "            if n.level and n.level > 0:\n",
    "                base_mod = _resolve_relative_module(this_module, n.level, n.module)\n",
    "            else:\n",
    "                base_mod = n.module\n",
    "            if not base_mod:\n",
    "                continue\n",
    "            for a in n.names:\n",
    "                local = a.asname if a.asname else a.name\n",
    "                # Could be a submodule or a symbol; we store as fully qualified\n",
    "                from_names[local] = f\"{base_mod}.{a.name}\"\n",
    "    return import_aliases, from_names\n",
    "\n",
    "# -----------------------------\n",
    "# main API\n",
    "# -----------------------------\n",
    "\n",
    "def extract_function_source_ast(\n",
    "    file_path: str | Path,\n",
    "    func_or_qualname: str,\n",
    "    include_helpers: bool = False,\n",
    "    *,\n",
    "    base_path: str | Path,\n",
    "    detailed_functions: bool = False,\n",
    "    recursive_helper: bool = False,\n",
    "    aggressive_fallback: bool = False,  # set True to allow cross-project name fallback\n",
    "    # tool_context: ToolContext\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract a function or method source by name.\n",
    "\n",
    "    Args:\n",
    "      file_path: Path to the file containing the target function/method.\n",
    "      func_or_qualname: \"foo\" or \"ClassName.method\".\n",
    "      include_helpers: If True, also return helper function *paths* discovered\n",
    "                       from calls inside the target, searching across the project.\n",
    "      base_path: Project root directory. Only files under this root are considered.\n",
    "      detailed_functions: If True, include detailed information about function arguments and return types.\n",
    "      recursive_helper: If True, include helper functions found in the same file.\n",
    "      aggressive_fallback: If True, when we can't prove a binding, include all\n",
    "                           same-named top-level functions found across the project.\n",
    "      tool_context: Tool context (optional for session actions).\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"code\": str,\n",
    "        \"start_line\": int,\n",
    "        \"end_line\": int,\n",
    "        \"function\": str,\n",
    "        \"file\": str,\n",
    "        \"helpers\": List[str]  # dotted function paths across the project\n",
    "      }\n",
    "    \"\"\"\n",
    "    base = Path(base_path).resolve()\n",
    "    path = Path(file_path).resolve()\n",
    "    src = path.read_text(encoding=\"utf-8\")\n",
    "    src_lines = src.splitlines()\n",
    "\n",
    "    mod = ast.parse(src)\n",
    "    top_funcs, class_methods = _gather_defs(mod)\n",
    "\n",
    "    class_name: Optional[str] = None\n",
    "    func_name = func_or_qualname\n",
    "    if \".\" in func_or_qualname:\n",
    "        class_name, func_name = func_or_qualname.split(\".\", 1)\n",
    "\n",
    "    target_node: Optional[FuncNode] = None\n",
    "    if class_name:\n",
    "        methods = class_methods.get(class_name, {})\n",
    "        target_node = methods.get(func_name)\n",
    "    else:\n",
    "        target_node = top_funcs.get(func_name)\n",
    "        if target_node is None:\n",
    "            for cls, methods in class_methods.items():\n",
    "                if func_name in methods:\n",
    "                    target_node = methods[func_name]\n",
    "                    class_name = cls\n",
    "                    break\n",
    "\n",
    "    if target_node is None:\n",
    "        available = sorted(list(top_funcs.keys()) + [f\"{c}.{m}\" for c, ms in class_methods.items() for m in ms])\n",
    "        raise ValueError(f\"Function '{func_or_qualname}' not found. Available: {available}\")\n",
    "\n",
    "    main_code, start, end = _slice_with_decorators(src_lines, target_node)\n",
    "    pieces = [f\"# Extracted from {path.name}:{start}-{end}\\n{main_code}\"]\n",
    "\n",
    "    helper_function_paths: List[str] = []\n",
    "    helper_function_paths_final = []\n",
    "    if include_helpers:\n",
    "        by_name, by_mod_func = _index_project_functions(str(base))\n",
    "\n",
    "        this_module = _to_module_qualname(base, path)\n",
    "        import_aliases, from_names = _parse_import_maps(mod, this_module)\n",
    "\n",
    "        # collect calls + bound locals in the function\n",
    "        bare_names, qual_calls, bound_locals = _collect_calls_and_locals(target_node)\n",
    "\n",
    "        resolved_funcs: set[str] = set()\n",
    "\n",
    "        # ---- Bare calls: helper() ----\n",
    "        for name in bare_names:\n",
    "            # If the name is locally bound (param/assignment/etc.), we can't safely resolve it.\n",
    "            if name in bound_locals:\n",
    "                continue\n",
    "\n",
    "            # Same-file top-level function wins\n",
    "            if name in top_funcs:\n",
    "                resolved_funcs.add(f\"{this_module}.{name}\")\n",
    "                continue\n",
    "\n",
    "            # from pkg.mod import name [as alias]\n",
    "            if name in from_names:\n",
    "                full = from_names[name]  # e.g., 'pkg.mod.helper'\n",
    "                if full in by_mod_func:\n",
    "                    resolved_funcs.add(full)\n",
    "                    continue\n",
    "\n",
    "            # No project-wide name scan unless explicitly allowed\n",
    "            if aggressive_fallback:\n",
    "                for _fp, module_name, _node in by_name.get(name, []):\n",
    "                    # skip the exact same target function identity\n",
    "                    if module_name == this_module and name == func_name:\n",
    "                        continue\n",
    "                    resolved_funcs.add(f\"{module_name}.{name}\")\n",
    "\n",
    "        # ---- Qualified calls: utils.helper(), pkg.sub.mod.helper() ----\n",
    "        for root, chain in qual_calls:\n",
    "            if not chain:\n",
    "                continue\n",
    "\n",
    "            # If root is locally bound, treat as object, not module\n",
    "            if root in bound_locals:\n",
    "                continue\n",
    "\n",
    "            func = chain[-1]\n",
    "            prefix = chain[:-1]\n",
    "\n",
    "            # Root can come from either 'import ... as root' OR 'from ... import root as root'\n",
    "            base_mod = import_aliases.get(root)\n",
    "            if not base_mod:\n",
    "                # If root was imported via 'from X import root', that map points to X.root\n",
    "                maybe = from_names.get(root)\n",
    "                if maybe:\n",
    "                    # If 'root' is actually a submodule imported via 'from X import root'\n",
    "                    base_mod = maybe\n",
    "\n",
    "            if not base_mod:\n",
    "                continue  # unknown root â†’ skip\n",
    "\n",
    "            full_mod = \".\".join([base_mod] + prefix) if prefix else base_mod\n",
    "            candidate = f\"{full_mod}.{func}\"\n",
    "\n",
    "            if candidate in by_mod_func:\n",
    "                resolved_funcs.add(candidate)\n",
    "            elif aggressive_fallback:\n",
    "                for _fp, module_name, _node in by_name.get(func, []):\n",
    "                    resolved_funcs.add(f\"{module_name}.{func}\")\n",
    "        helper_function_paths = sorted(resolved_funcs)\n",
    "        if detailed_functions:\n",
    "            for func_path in helper_function_paths:\n",
    "                p, f = create_file_path(base_path, func_path)\n",
    "                # print(f, p)\n",
    "                helper_function_paths_final.append(extract_function_source_ast(\n",
    "                    file_path=p,\n",
    "                    func_or_qualname=f,\n",
    "                    include_helpers=detailed_functions,\n",
    "                    base_path=base_path,\n",
    "                    detailed_functions=recursive_helper,\n",
    "                ))\n",
    "        else:\n",
    "            helper_function_paths_final = helper_function_paths\n",
    "\n",
    "    return {\n",
    "        \"code\": \"\\n\".join(pieces),\n",
    "        \"start_line\": start,\n",
    "        \"end_line\": end,\n",
    "        \"function\": func_or_qualname,\n",
    "        \"file\": str(path),\n",
    "        \"helpers\": helper_function_paths_final,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c4ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator, model_validator  # Pydantic v2\n",
    "\n",
    "class ParameterInputSchema(BaseModel):\n",
    "    function_path: str = Field(str, alias=\"function_path\")\n",
    "    include_helpers: bool = Field(False, alias=\"include_helpers\")\n",
    "    base_path: str = Field(str, alias=\"base_path\")\n",
    "    detailed_functions: bool = Field(False, alias=\"detailed_functions\")\n",
    "    recursive_helper: bool = Field(False, alias=\"recursive_helper\")\n",
    "    aggressive_fallback: bool = Field(False, alias=\"aggressive_fallback\")\n",
    "\n",
    "    # allow using field names instead of aliases and vice-versa\n",
    "    model_config = dict(populate_by_name=True)\n",
    "\n",
    "    # @field_validator(\"base_path\", mode=\"before\")\n",
    "    # @classmethod\n",
    "    # def _coerce_to_path(cls, v):\n",
    "    #     return Path(v).expanduser() if not isinstance(v, Path) else v\n",
    "\n",
    "    # @model_validator(mode=\"after\")\n",
    "    # def _validate_paths(self):\n",
    "    #     # Convert to Path\n",
    "    #     self.base_path = Path(self.base_path).resolve()\n",
    "    #     self.file_path = Path(self.file_path).resolve()\n",
    "\n",
    "    #     if not self.file_path.exists():\n",
    "    #         raise ValueError(f\"file_path does not exist: {self.file_path}\")\n",
    "    #     if not self.file_path.is_file():\n",
    "    #         raise ValueError(\"file_path must be a file\")\n",
    "\n",
    "    #     # Ensure file_path is within base_path\n",
    "    #     try:\n",
    "    #         self.file_path.relative_to(self.base_path)\n",
    "    #     except ValueError as e:\n",
    "    #         raise ValueError(\n",
    "    #             f\"file_path must be under base_path\\n  file: {self.file_path}\\n  base: {self.base_path}\"\n",
    "    #         ) from e\n",
    "    #     return self\n",
    "\n",
    "    # Back-compat property for code that still references the misspelling\n",
    "    @property\n",
    "    def reursive_helper(self) -> bool:\n",
    "        return self.recursive_helper\n",
    "\n",
    "    def to_kwargs(self) -> Dict[str, Any]:\n",
    "        \"\"\"Map schema to extract_function_source_ast kwargs (preserves original param names).\"\"\"\n",
    "        path, func = create_file_path(str(self.base_path), str(self.function_path))\n",
    "        return {\n",
    "            \"file_path\": path,\n",
    "            \"func_or_qualname\": func,\n",
    "            \"include_helpers\": self.include_helpers,\n",
    "            \"base_path\": str(self.base_path),\n",
    "            \"detailed_functions\": self.detailed_functions,\n",
    "            \"recursive_helper\": self.recursive_helper,   # keep original name expected by your function\n",
    "            \"aggressive_fallback\": self.aggressive_fallback,\n",
    "        }\n",
    "\n",
    "def extract_function_source(\n",
    "        params: ParameterInputSchema,  # set True to allow cross-project name fallback\n",
    "        # tool_context: ToolContext\n",
    "    ) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Wrapper around `extract_function_source_ast` that accepts a validated\n",
    "    `ParameterInputSchema` and forwards its fields as keyword arguments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : ParameterInputSchema\n",
    "        Contains:\n",
    "          - function_path (str): Path to the function/method to extract.\n",
    "          - include_helpers (bool): If True, also return helper function *paths* discovered\n",
    "            from calls inside the target, searching across the project.\n",
    "          - base_path (str): Project root directory. Only files under this root are considered.\n",
    "          - detailed_functions (bool): If True, include detailed information about function\n",
    "            arguments and return types.\n",
    "          - recursive_helper (bool): If True, include helper functions found in the same file.\n",
    "          - aggressive_fallback (bool): If True, when binding can't be proven, include all\n",
    "            same-named top-level functions found across the project.\n",
    "    tool_context : ToolContext\n",
    "        Tool context (e.g., session/runtime context) passed through to the extractor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        {\n",
    "          \"code\": str,\n",
    "          \"start_line\": int,\n",
    "          \"end_line\": int,\n",
    "          \"function\": str,\n",
    "          \"file\": str,\n",
    "          \"helpers\": List[str] | List[Dict[str, Any]]  # depends on detailed_helpers flags\n",
    "        }\n",
    "    \"\"\"\n",
    "    return extract_function_source_ast(\n",
    "        # tool_context=tool_context,\n",
    "        **params.to_kwargs(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1d07c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JonathanChackoPattas\\OneDrive - Maritime Support Solutions\\Desktop\\MSS-Automation\\Inventory\\views_pack\\terminal.py \n",
      "saudi_tsr_output_checker()\n",
      "{\n",
      "  \"code\": \"# Extracted from terminal.py:455-583\\n@csrf_exempt\\ndef saudi_tsr_output_checker(request):\\n    if request.method == 'POST':\\n        try:\\n            report = request.POST.get('document_type')\\n            file_path = request.FILES.get('file')\\n            if not file_path:\\n                return JsonResponse({\\\"status\\\": \\\"error\\\", \\\"message\\\": \\\"No file uploaded.\\\", \\\"data\\\": pd.DataFrame().to_dict()}, status=400)\\n            TSR = True\\n            if False:\\n                pass\\n            ## DAMMAM DEPOTS\\n            elif report == \\\"GLOBE-DAMMAM_Report\\\":\\n                df = GLOBE_DAMMAM(file_path)\\n                \\\"\\\"\\\"\\n                returns ['CONTAINER_NUMBER','RCVC_DATE', 'SNTS_DATE']\\n                \\\"\\\"\\\"\\n            elif report == \\\"ALI-RAZA_Report\\\":\\n                df = ALI_RAZA_DAMMAM(file_path)\\n                \\\"\\\"\\\"\\n                returns ['CONTAINER_NUMBER','RCVC_DATE', 'SNTS_DATE']\\n                \\\"\\\"\\\"\\n            ## JEDDAH DEPOTS\\n            elif report == \\\"GLOBE-JEDDAH_Report\\\":\\n                df = GLOBE_JEDDAH(file_path)\\n                \\\"\\\"\\\"\\n                returns \\n                    ['CONTAINER_NUMBER', 'SNTC_DATE'] for .txt files \\n                    OR\\n                    ['CONTAINER_NUMBER','RCVC_DATE', 'SNTS_DATE'] for .xlsx files\\n                \\\"\\\"\\\"\\n            elif report == \\\"BAHRI-JEDDAH_Report\\\": # NOTE\\n                df = BAHRI_JEDDAH(file_path)\\n                \\\"\\\"\\\"\\n                returns ['CONTAINER_NUMBER','RCVC_DATE', 'SNTS_DATE']\\n                \\\"\\\"\\\"\\n            elif report == \\\"VISION-JEDDAH_Report\\\": # TODO\\n                df = VISION_JEDDAH(file_path)\\n                \\\"\\\"\\\"\\n                returns ['CONTAINER_NUMBER','RCVC_DATE', 'SNTS_DATE']\\n                \\\"\\\"\\\"\\n            ## DAMMAM TERMINALS\\n            elif report == \\\"SGP_Report\\\":\\n                df = SGP_TSR_Main(\\n                    path=file_path,\\n                    # tsr_dict=TSR_DICT\\n                )\\n                \\\"\\\"\\\"\\n                returns ['LINE', 'BL_NUMBER', 'CONTAINER_NUMBER', 'CONTAINER_SIZE', 'EMPTY_FULL', 'PORT', 'CONTAINER_TYPE', 'PROCESS']\\n                \\\"\\\"\\\"\\n                df['LINE'] = df['LINE'].map(NVOCCandLINEdf().set_index('SGP-DMM')['NVOCC'].to_dict())\\n            ## JEDDAH TERMINALS\\n            elif report == \\\"RSGT-TSR_Report\\\":\\n                df = RSGT_TSR_Main(\\n                    path=file_path,\\n                    # tsr_dict=TSR_DICT\\n                )\\n                \\\"\\\"\\\"\\n                returns ['LINE', 'BL_NUMBER', 'CONTAINER_NUMBER', 'CONTAINER_SIZE', 'EMPTY_FULL', 'PORT', 'CONTAINER_TYPE', 'PROCESS']\\n                \\\"\\\"\\\"\\n                df['LINE'] = df['LINE'].map(NVOCCandLINEdf().set_index('RSGT')['NVOCC'].to_dict())\\n            elif report == \\\"DPW_Report\\\":\\n                df, error, message = DPW_TSR_Main(\\n                    path=file_path,\\n                    # tsr_dict=TSR_DICT\\n                )\\n                \\\"\\\"\\\"\\n                returns ['LINE', 'BL_NUMBER', 'CONTAINER_NUMBER', 'CONTAINER_SIZE', 'EMPTY_FULL', 'PORT', 'CONTAINER_TYPE', 'PROCESS'], error, message\\n                \\\"\\\"\\\"\\n                if error:\\n                    return JsonResponse({\\\"status\\\": \\\"error\\\", \\\"message\\\": message}, status=400)\\n                # else:\\n                #     pass\\n                df['LINE'] = df['LINE'].map(NVOCCandLINEdf().set_index('SCT/DP WORLD')['NVOCC'].to_dict())\\n            # elif report == \\\"MandM_Report\\\":\\n            #     df = MandM_TSR_Main(\\n            #         path=file_path,\\n            #         tsr_dict=TSR_DICT\\n            #     )\\n            ## OTHER REPORTS\\n            elif report == \\\"RSGT-DAILY_Report\\\":\\n                df = RSGT_Daily(file_path)\\n                \\\"\\\"\\\"\\n                returns ['CONTAINER_NUMBER','SNTC_DATE','RCVS_DATE']\\n                \\\"\\\"\\\"\\n            elif report == \\\"RSGT-UIY_Report\\\":\\n                df, Special_UIY_forVV_DF = RSGT_UnitInYard(file_path)\\n                \\\"\\\"\\\"\\n                returns ['CONTAINER_NUMBER', 'RCVS_DATE'], ['CONTAINER_NUMBER', 'ISO', 'SIZE', 'TYPE', 'STATUS', 'CARRIER_REFERENCE', 'INBOUND', 'INDATE', 'POD', 'BL_NUMBER', 'CATEGORY', 'LINE']\\n                \\\"\\\"\\\"\\n            # elif report == \\\"Common_Report\\\":\\n            #     df = Common_Report()\\n            #     \\\"\\\"\\\"\\n            #     returns ['CONTAINER_NUMBER', 'SNTC_DATE', 'RCVC_DATE', 'SNTS_DATE', 'RCVS_DATE']\\n            #     \\\"\\\"\\\"\\n            # ## SCRAPPERS\\n            # elif report in [\\\"SGP_Scrapper\\\", \\\"DPW_Scrapper\\\"]:\\n            #     obj = ScrappersMain()\\n            #     if report == \\\"SGP_Scrapper\\\":\\n            #         df = SecondaryData(\\n            #             df=obj.START(SITE=\\\"SGP_Services_Saudi\\\"), \\n            #             session_id=session_id\\n            #         )\\n            #     elif report == \\\"DPW_Scrapper\\\":\\n            #         df = SecondaryData(\\n            #             df=obj.START(SITE=\\\"DP_World_Saudi\\\"), \\n            #             session_id=session_id\\n            #         )\\n            else:\\n                df = pd.DataFrame(columns=['CONTAINER_NUMBER', 'SNTC_DATE', 'RCVC_DATE', 'SNTS_DATE', 'RCVS_DATE']) if not TSR else pd.DataFrame()\\n            if df.empty:\\n                return JsonResponse({\\\"status\\\": \\\"error\\\", \\\"message\\\": \\\"No data found in the file.\\\", \\\"data\\\": df.to_dict()}, status=400)\\n            else:\\n                rows = []\\n                for row in df.to_dict(orient='records'):\\n                    for keys in row.keys():\\n                        if row[keys] in [np.nan, None, \\\"\\\"]:\\n                            row[keys] = None\\n                        elif str(row[keys]) == \\\"NaT\\\":\\n                            row[keys] = None\\n                        # else:\\n                        #     pass\\n                    rows.append(row)\\n                df = pd.DataFrame(rows)\\n                return JsonResponse({\\\"status\\\": \\\"success\\\", \\\"message\\\": \\\"Data processed successfully.\\\", \\\"data\\\": df.to_dict()}, status=200)\\n        except Exception as e:\\n            serverPrint(request, e)\\n            return JsonResponse({\\\"status\\\": \\\"error\\\", \\\"message\\\": str(e), \\\"data\\\": pd.DataFrame().to_dict()}, status=500)\\n    return JsonResponse({\\\"status\\\": \\\"error\\\", \\\"message\\\": \\\"Invalid request method.\\\", \\\"data\\\": pd.DataFrame().to_dict()}, status=400)\",\n",
      "  \"start_line\": 455,\n",
      "  \"end_line\": 583,\n",
      "  \"function\": \"saudi_tsr_output_checker\",\n",
      "  \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Inventory\\\\views_pack\\\\terminal.py\",\n",
      "  \"helpers\": [\n",
      "    {\n",
      "      \"code\": \"# Extracted from DepotModule.py:116-131\\ndef ALI_RAZA_DAMMAM(path):\\n    df = pd.read_excel(path, f\\\"Sheet1\\\")\\n    # df.columns = df.iloc[1]\\n    df = df[1:]\\n    df = df.reset_index()\\n    # RCVC\\n    clmns,rowz = {\\\"Container Number\\\":\\\"CONTAINER_NUMBER\\\", \\\"Gate In Date\\\":\\\"RCVC_DATE\\\"}, [\\\"CONTAINER_NUMBER\\\",\\\"RCVC_DATE\\\"]\\n    df[\\\"Gate In Date\\\"] = pd.to_datetime(df[\\\"Gate In Date\\\"], format=\\\"%d-%b-%Y %H:%M:%S\\\")\\n    if \\\"Gate Out Date\\\" in df.columns: # SNTS\\n        clmns.update({\\\"Gate Out Date\\\":\\\"SNTS_DATE\\\"})\\n        # rowz.remove(\\\"RCVC_DATE\\\") # TODO-NOTE KEEP OR NOT?!\\n        rowz.append(\\\"SNTS_DATE\\\")\\n        df[\\\"Gate Out Date\\\"] = pd.to_datetime(df[\\\"Gate Out Date\\\"], format=\\\"%d-%b-%Y %H:%M:%S\\\")\\n    df = df.rename(columns=clmns)\\n    df = df[rowz]\\n    return df\",\n",
      "      \"start_line\": 116,\n",
      "      \"end_line\": 131,\n",
      "      \"function\": \"ALI_RAZA_DAMMAM\",\n",
      "      \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Inventory\\\\EXE_Extras\\\\DepotModule.py\",\n",
      "      \"helpers\": []\n",
      "    },\n",
      "    {\n",
      "      \"code\": \"# Extracted from DepotModule.py:58-73\\ndef BAHRI_JEDDAH(path):\\n    df = pd.read_excel(path) # , r\\\"Page1\\\"\\n    df.columns = df.iloc[4]\\n    df = df[5:-2]\\n    df = df.reset_index()\\n    # RCVC\\n    clmns,rowz = {\\\"Container\\\":\\\"CONTAINER_NUMBER\\\", \\\"Gate-In Date\\\":\\\"RCVC_DATE\\\"}, [\\\"CONTAINER_NUMBER\\\",\\\"RCVC_DATE\\\"]\\n    df[\\\"Gate-In Date\\\"] = pd.to_datetime(df[\\\"Gate-In Date\\\"], format=\\\"%d-%m-%Y %H:%M\\\").apply(lambda x: None if pd.isnull(x) else x)#.apply(lambda x: None if x == pd.NaT else x)\\n    if \\\"Gate-Out Date\\\" in df.columns: # SNTS\\n        clmns.update({\\\"Gate-Out Date\\\":\\\"SNTS_DATE\\\"})\\n        # rowz.remove(\\\"RCVC_DATE\\\") # TODO-NOTE KEEP OR NOT?!\\n        rowz.append(\\\"SNTS_DATE\\\")\\n        df[\\\"Gate-Out Date\\\"] = pd.to_datetime(df[\\\"Gate-Out Date\\\"], format=\\\"%d-%m-%Y %H:%M\\\").apply(lambda x: None if pd.isnull(x) else x)#.apply(lambda x: None if x == pd.NaT else x)\\n    df = df.rename(columns=clmns)\\n    df = df[rowz]\\n    return df\",\n",
      "      \"start_line\": 58,\n",
      "      \"end_line\": 73,\n",
      "      \"function\": \"BAHRI_JEDDAH\",\n",
      "      \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Inventory\\\\EXE_Extras\\\\DepotModule.py\",\n",
      "      \"helpers\": []\n",
      "    },\n",
      "    {\n",
      "      \"code\": \"# Extracted from DepotModule.py:98-114\\ndef GLOBE_DAMMAM(path):\\n    df = pd.read_excel(path, f\\\"Sheet1\\\")\\n    df.columns = df.iloc[1]\\n    df = df[2:]\\n    df = df.reset_index()\\n    df[\\\"CONTR\\\"] = df[\\\"CONTR\\\"].replace(' ',\\\"\\\").replace(re.compile(r\\\"\\\\s\\\"), '')\\n    # RCVC\\n    clmns,rowz = {\\\"CONTR\\\":\\\"CONTAINER_NUMBER\\\", \\\"IN_DATETIME\\\":\\\"RCVC_DATE\\\"}, [\\\"CONTAINER_NUMBER\\\",\\\"RCVC_DATE\\\"]\\n    df[\\\"IN_DATETIME\\\"] = pd.to_datetime(df[\\\"IN_DATETIME\\\"], format='%d/%b/%Y %H:%M:%S')\\n    if \\\"OUT_DATETIME\\\" in df.columns: # SNTS\\n        clmns.update({\\\"OUT_DATETIME\\\":\\\"SNTS_DATE\\\"})\\n        # rowz.remove(\\\"RCVC_DATE\\\") # TODO-NOTE KEEP OR NOT?!\\n        rowz.append(\\\"SNTS_DATE\\\")\\n        df[\\\"OUT_DATETIME\\\"] = pd.to_datetime(df[\\\"OUT_DATETIME\\\"], format='%d/%b/%Y %H:%M:%S')\\n    df = df.rename(columns=clmns)\\n    df = df[rowz]\\n    return df\",\n",
      "      \"start_line\": 98,\n",
      "      \"end_line\": 114,\n",
      "      \"function\": \"GLOBE_DAMMAM\",\n",
      "      \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Inventory\\\\EXE_Extras\\\\DepotModule.py\",\n",
      "      \"helpers\": []\n",
      "    },\n",
      "    {\n",
      "      \"code\": \"# Extracted from DepotModule.py:5-56\\ndef GLOBE_JEDDAH(path):\\n    file_extension = os.path.splitext(path)[1]\\n    if file_extension in CapitalVariations(\\\".txt\\\"):\\n        with open(path, 'r') as file:\\n            file_content = file.read()\\n            file_content = file_content.replace('\\\"','')\\n            lines = file_content.strip().split('\\\\n')\\n            data = [line.split('\\\\t') for line in lines]\\n            data[-1].append('')\\n            data[-1].append('')\\n            df = pd.DataFrame(data, columns=[\\n                'SNTC_DATE', # Email Confirmed by @Shivani on 29th July 2024\\n                'CONTAINER_NUMBER',\\n                'CONTAINER_SIZE_TYPE',\\n                'ALT_DATE',\\n                'Unkown00',\\n                'Unkown01',\\n                'Unkown02',\\n                'Unkown03',\\n                'Unkown04',\\n                'Unkown05',\\n                'Unkown06',\\n                'Unkown07',\\n                'Unkown08',\\n                'Unkown09',\\n                'Unkown10',\\n                'Unkown11'\\n            ])\\n        # for replace in rowz:\\n        #     if replace != \\\"CONTAINER_NUMBER\\\":\\n        #             rowz.remove(replace) # [item for item in my_list if item != value_to_remove]\\n        if \\\"CONTAINER_NUMBER\\\" in df.columns:\\n            df[\\\"CONTAINER_NUMBER\\\"] = df[\\\"CONTAINER_NUMBER\\\"].str.replace(' ','') # astype(str)\\n        if \\\"CONTAINER_SIZE_TYPE\\\" in df.columns:\\n            df[['CONTAINER_SIZE', 'CONTAINER_TYPE']] = df['CONTAINER_SIZE_TYPE'].str.extract(r'(\\\\d{2})(.*)') # .extract(r'(\\\\d+)([A-Z_]+)')\\n            # df.drop(columns=['CONTAINER_SIZE_TYPE'], inplace=True)\\n        df = df[['CONTAINER_NUMBER', 'SNTC_DATE']] # ,'CONTAINER_SIZE', 'CONTAINER_TYPE', 'ALT_DATE'\\n    else:\\n        df = pd.read_excel(path, f\\\"Sheet1\\\")\\n        df.columns = df.iloc[0]\\n        df = df[1:]\\n        clmns, rowz = {\\\"CONTAINER\\\":\\\"CONTAINER_NUMBER\\\", \\\"IN DATETIME\\\":\\\"RCVC_DATE\\\"}, [\\\"CONTAINER_NUMBER\\\",\\\"RCVC_DATE\\\"]\\n        # RCVC\\n        df[\\\"IN DATETIME\\\"] = pd.to_datetime(df[\\\"IN DATETIME\\\"], format='%d/%b/%Y %H:%M:%S')\\n        if \\\"OUT DATETIME\\\" in df.columns: # SNTS\\n            clmns.update({\\\"OUT DATETIME\\\":\\\"SNTS_DATE\\\"})\\n            # rowz.remove(\\\"RCVC_DATE\\\") # TODO-NOTE KEEP OR NOT?!\\n            rowz.append(\\\"SNTS_DATE\\\")\\n            df[\\\"OUT DATETIME\\\"] = pd.to_datetime(df[\\\"OUT DATETIME\\\"], format='%d/%b/%Y %H:%M:%S')\\n        df = df.rename(columns=clmns)\\n        df = df[rowz]\\n    return df\",\n",
      "      \"start_line\": 5,\n",
      "      \"end_line\": 56,\n",
      "      \"function\": \"GLOBE_JEDDAH\",\n",
      "      \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Inventory\\\\EXE_Extras\\\\DepotModule.py\",\n",
      "      \"helpers\": []\n",
      "    },\n",
      "    {\n",
      "      \"code\": \"# Extracted from DepotModule.py:75-96\\ndef VISION_JEDDAH(path):\\n    df = pd.read_excel(path) # , r\\\"Sheet1\\\"\\n    # for idx in range(len(df)):\\n    #     if 'Container No.' in df.iloc[idx].to_dict().values():\\n    #         startAt = idx\\n    #         break\\n    startAt = 9 if 'Container No.' in df.iloc[9].to_list() else 8\\n    df.columns = df.iloc[startAt] # df.columns = df.iloc[2]\\n    df = df[startAt+2:] # df = df[4:]\\n    df = df.dropna(subset=['Container No.']) # 'Container'\\n    df = df.reset_index()\\n    # RCVC | GATE-IN DATE\\n    clmns,rowz = {\\\"Container No.\\\":\\\"CONTAINER_NUMBER\\\", \\\"Gate-In Date\\\":\\\"RCVC_DATE\\\"}, [\\\"CONTAINER_NUMBER\\\",\\\"RCVC_DATE\\\"]\\n    df[\\\"Gate-In Date\\\"] = pd.to_datetime(df[\\\"Gate-In Date\\\"], format=\\\"%Y-%m-%d %H:%M:%S\\\").apply(lambda x: None if pd.isnull(x) else x)#.apply(lambda x: None if x == pd.NaT else x)\\n    if \\\"Gate-Out Date\\\" in df.columns: # SNTS | GATE-OUT DATE\\n        clmns.update({\\\"Gate-Out Date\\\":\\\"SNTS_DATE\\\"})\\n        # rowz.remove(\\\"RCVC_DATE\\\") # TODO-NOTE KEEP OR NOT?!\\n        rowz.append(\\\"SNTS_DATE\\\")\\n        df[\\\"Gate-Out Date\\\"] = pd.to_datetime(df[\\\"Gate-Out Date\\\"], format=\\\"%Y-%m-%d %H:%M:%S\\\").apply(lambda x: None if pd.isnull(x) else x)#.apply(lambda x: None if x == pd.NaT else x)\\n    df = df.rename(columns=clmns)\\n    df = df[rowz]\\n    return df\",\n",
      "      \"start_line\": 75,\n",
      "      \"end_line\": 96,\n",
      "      \"function\": \"VISION_JEDDAH\",\n",
      "      \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Inventory\\\\EXE_Extras\\\\DepotModule.py\",\n",
      "      \"helpers\": []\n",
      "    },\n",
      "    {\n",
      "      \"code\": \"# Extracted from TerminalModule.py:14-38\\ndef RSGT_Daily(path):\\n    # SNTC\\n    Df_SNTC = pd.read_excel(path, f\\\"GATE_OUT\\\")\\n    Df_SNTC = Df_SNTC.iloc[4:, :]\\n    Df_SNTC.columns = Df_SNTC.iloc[0]\\n    Df_SNTC = Df_SNTC[1:]\\n    Df_SNTC = Df_SNTC.reset_index()\\n    Df_SNTC = Df_SNTC.rename(columns={\\\"CONTAINER\\\":\\\"CONTAINER_NUMBER\\\", \\\"DATE DELIVERED\\\":\\\"SNTC_DATE\\\"})\\n    Df_SNTC = Df_SNTC[[\\\"CONTAINER_NUMBER\\\",\\\"SNTC_DATE\\\"]]\\n    Df_SNTC.dropna(inplace=True)\\n    Df_SNTC[\\\"SNTC_DATE\\\"] = pd.to_datetime(Df_SNTC[\\\"SNTC_DATE\\\"]) if not Df_SNTC.empty else None\\n    # RCVS\\n    Df_RCVS = pd.read_excel(path, f\\\"GATE_IN\\\")\\n    Df_RCVS = Df_RCVS.iloc[2:, :]\\n    Df_RCVS.columns = Df_RCVS.iloc[0]\\n    Df_RCVS = Df_RCVS[1:]\\n    Df_RCVS = Df_RCVS.reset_index()\\n    Df_RCVS = Df_RCVS.rename(columns={\\\"CONTAINER\\\":\\\"CONTAINER_NUMBER\\\", 'DATE ':\\\"RCVS_DATE\\\"})\\n    Df_RCVS = Df_RCVS[[\\\"CONTAINER_NUMBER\\\",\\\"RCVS_DATE\\\"]]\\n    Df_RCVS.dropna(inplace=True)\\n    Df_RCVS[\\\"RCVS_DATE\\\"] = pd.to_datetime(Df_RCVS[\\\"RCVS_DATE\\\"]) if not Df_RCVS.empty else None\\n    # Merge DataFrames on 'CONTAINER_NUMBER'\\n    merged_df = pd.merge(Df_SNTC, Df_RCVS, on='CONTAINER_NUMBER', how='outer')\\n    merged_df = merged_df[[\\\"CONTAINER_NUMBER\\\",\\\"SNTC_DATE\\\",\\\"RCVS_DATE\\\"]]\\n    return merged_df\",\n",
      "      \"start_line\": 14,\n",
      "      \"end_line\": 38,\n",
      "      \"function\": \"RSGT_Daily\",\n",
      "      \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Inventory\\\\EXE_Extras\\\\TerminalModule.py\",\n",
      "      \"helpers\": []\n",
      "    },\n",
      "    {\n",
      "      \"code\": \"# Extracted from TerminalModule.py:40-93\\ndef RSGT_UnitInYard(path):\\n    DF = pd.read_excel(path, r\\\"Unit In Yard\\\") # r\\\"Unit In Yard Export\\\"\\n    DF = DF.iloc[3:, :]\\n    DF.columns = DF.iloc[0]\\n    DF = DF[1:]\\n    DF = DF.reset_index()\\n    # RSGTmasterCompare = [value for value in MasterDF[\\\"RSGT\\\"].to_list() if value != '']\\n    # DF = DF[DF['Line'].isin(RSGTmasterCompare)]\\n    col_map = {\\n        'Container': 'CONTAINER_NUMBER',\\n        'ISO': 'ISO',\\n        'Size': 'SIZE',\\n        'Type': 'TYPE',\\n        'Status': 'STATUS',\\n        'CARRIER REFERENCE': 'CARRIER_REFERENCE',\\n        'INBOUND': 'INBOUND',\\n        'Indate': 'INDATE',\\n        'POD': 'POD',\\n        'BL Nbr': 'BL_NUMBER',\\n        'Category': 'CATEGORY',\\n        'Line': 'LINE',\\n    }\\n    available = [col for col in col_map.keys() if col in DF.columns]\\n    DF = DF.rename(columns=col_map) # DF[available].rename(columns=col_map)\\n    for vsl_col in ['CARRIER_REFERENCE', 'INBOUND']:\\n        if vsl_col in DF.columns:\\n            DF[vsl_col] = DF[vsl_col].apply(\\n                lambda x: (\\n                    x.replace('Out:V ', '') if isinstance(x, str) and x.startswith('Out:V ')\\n                    else 'DUMMY_RSGT' if isinstance(x, str) and x.startswith('In:V ')\\n                    else x # .strip() if isinstance(x, str) else x\\n                )\\n            )\\n            # DF[vsl_col] = DF[vsl_col].replace('DUMMY_RSGT', None)\\n    available = [col for col in col_map.values() if col in DF.columns]\\n    pass\\n    # Used to Extract RCVS\\n    Df_Export = DF[DF['STATUS'].isin(['EXPRT EMPTY', 'EXPRT FULL'])] # pd.read_excel(path, r\\\"Unit In Yard Export\\\")\\n    Df_Export = Df_Export.rename(columns={'INDATE':\\\"RCVS_DATE\\\"})\\n    Df_Export = Df_Export[[\\\"CONTAINER_NUMBER\\\", \\\"RCVS_DATE\\\"]]\\n    Df_Export.dropna(inplace=True)\\n    Df_Export[\\\"RCVS_DATE\\\"] = pd.to_datetime(Df_Export[\\\"RCVS_DATE\\\"], errors='coerce') if not Df_Export.empty else None\\n    merged_df = Df_Export # Merge DataFrames on 'CONTAINER_NUMBER'\\n    merged_df = merged_df[[\\\"CONTAINER_NUMBER\\\", \\\"RCVS_DATE\\\"]]\\n    pass\\n    # Used to Extract ____ (Possibly Discharge Date)\\n    Df_Import = DF[DF['STATUS'].isin(['IMPRT EMPTY', 'IMPRT FULL'])]\\n    pass\\n    # Normally Used to Extract TRSHP Vessel Voyage and IN/OUT STATUS\\n    DF = DF[available]\\n    # DF = DF[DF['STATUS'].isin(['TRSHP EMPTY', 'TRSHP FULL'])]\\n    DF['INDATE'] = pd.to_datetime(DF['INDATE'], errors='coerce', dayfirst=True) if 'INDATE' in DF.columns else None # Added `dayfirst=True` cause without it was Nullifying Data.\\n    pass\\n    return merged_df, DF\",\n",
      "      \"start_line\": 40,\n",
      "      \"end_line\": 93,\n",
      "      \"function\": \"RSGT_UnitInYard\",\n",
      "      \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Inventory\\\\EXE_Extras\\\\TerminalModule.py\",\n",
      "      \"helpers\": []\n",
      "    },\n",
      "    {\n",
      "      \"code\": \"# Extracted from Terminal_Common.py:95-99\\ndef NVOCCandLINEdf():\\n    NVOCCandLINEcodeFile = os.path.join(settings.BASE_DIR, \\\"SAVED_FILES\\\", \\\"STACK_Details_saudi.xlsx\\\")\\n    df = pd.read_excel(NVOCCandLINEcodeFile)\\n    df = df.filter(['NVOCC' , 'RSGT', 'SCT/DP WORLD', 'SGP-DMM'])\\n    return df\",\n",
      "      \"start_line\": 95,\n",
      "      \"end_line\": 99,\n",
      "      \"function\": \"NVOCCandLINEdf\",\n",
      "      \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Inventory\\\\EXE_Extras\\\\Terminal_Common.py\",\n",
      "      \"helpers\": []\n",
      "    },\n",
      "    {\n",
      "      \"code\": \"# Extracted from Terminal_Main.py:233-335\\ndef DPW_TSR_Main(path): # , tsr_dict\\n    DPWmasterCompare = [value for value in MasterDF[\\\"SCT/DP WORLD\\\"].to_list() if value != '']\\n    CSV_Use = False if os.path.splitext(path)[1] == \\\".pdf\\\" else True\\n    error = False\\n    message = \\\"\\\"\\n    if CSV_Use:\\n        try:\\n            merged_dfs = pd.read_csv(path) # , encoding='utf-8'\\n        except Exception as e: # except UnicodeDecodeError as e:\\n            merged_dfs = pd.read_excel(path)\\n        finally:\\n            # print(\\\"--------------------------------------\\\")\\n            # print(merged_dfs)\\n            pass\\n        # print(\\\"--------------------------------------\\\")\\n        # print(path)\\n        # print(\\\"--------------------------------------\\\")\\n        if 'S.No' in merged_dfs.columns: # Starting CSV\\n            if os.path.splitext(path)[1] != \\\".xlsx\\\": # in [\\\".xlsx\\\",\\\".xls\\\",\\\".csv\\\"]:\\n                merged_dfs = merged_dfs.iloc[:merged_dfs[merged_dfs['S.No'] == 'Operator'].index[0] -2]\\n            if 'Load Date' in merged_dfs.columns.to_list():\\n                merged_dfs.rename(columns={\\\"Load Date\\\":\\\"Discharge Time\\\"}, inplace=True)\\n        else:\\n            print(\\\"Please Provide the Alternative PDF Format\\\")\\n            error = True\\n    else: # Starting PDF\\n        rpns, singleContainer = DPW_PDF_SCRAPPER(path)\\n        for i in range(len(rpns)):\\n            for key, value in rpns[i].items():\\n                rpns[i][key] = value.rstrip() if value is not None else \\\"\\\"\\n        merged_dfs = pd.DataFrame(rpns) # pd.DataFrame([rpns])\\n        # merged_dfs[ ] = merged_dfs[ ].astype(str).str.replace(\\\" \\\",\\\"\\\").replace(' ','').replace('\\\\n','').replace('\\\\t','').replace(' ','').replace(\\\" \\\",\\\"\\\")\\n        merged_dfs.rename(columns={\\\"Size_Type\\\":\\\"Size/Type\\\"}, inplace=True)\\n        if not singleContainer:\\n            error = True\\n    if error:\\n        message = f\\\"Provide Alternative {'PDF' if CSV_Use else 'CSV'}\\\"\\n        return pd.DataFrame(columns=['LINE', 'BL_NUMBER', 'CONTAINER_NUMBER', 'CONTAINER_SIZE', 'EMPTY_FULL', 'PORT', 'CONTAINER_TYPE', 'PROCESS']), error, message\\n    try:\\n        merged_dfs = merged_dfs[['Line', 'Bl No', 'Container', 'Status', 'Size/Type', 'ISO']] # , 'Line', 'Size/Type', 'Load Position', 'Damage', 'Bl No', 'Discharge Time'\\n    except Exception as FORMAT_ERROR:\\n        error, message = True, str(FORMAT_ERROR)\\n        return pd.DataFrame(columns=['LINE', 'BL_NUMBER', 'CONTAINER_NUMBER', 'CONTAINER_SIZE', 'EMPTY_FULL', 'PORT', 'CONTAINER_TYPE', 'PROCESS']), error, message\\n    else:\\n        merged_dfs.rename(columns={\\\"Container\\\":\\\"CONTAINER_NUMBER\\\", \\\"Bl No\\\":\\\"BL_NUMBER\\\", \\\"Line\\\":\\\"LINE\\\"}, inplace=True)\\n        # I/E/T\\n        merged_dfs['PROCCESS_TYPE'] = np.select([merged_dfs['Status'].isin(['I/E', 'I/F', 'I/M']), merged_dfs['Status'].isin(['E/E', 'E/F', 'E/M']), merged_dfs['Status'].isin(['T/E', 'T/F', 'T/M'])], ['IMPORT', 'EXPORT', 'TRSHP'], default='') # merged_dfs['PROCCESS_TYPE'] = merged_dfs['Status'].apply(lambda x: 'IMPORT' if x in ['I/E', 'I/F'] else ('EXPORT' if x in ['E/E', 'E/F'] else ('TRSHP' if x in ['T/E', 'T/F'] else '')))\\n        # SIZE & TYPE\\n        try:\\n            merged_dfs['ISO'] = merged_dfs['ISO'].astype(int)\\n        except Exception as TYPEintError:\\n            print(TYPEintError)\\n        valid_iso = merged_dfs['Size/Type'].notna() & merged_dfs['Size/Type'].astype(str).str.strip().ne('') & (merged_dfs['Size/Type'].astype(str).str.len() == 4)\\n        merged_dfs['CONTAINER_SIZE'] = merged_dfs['ISO']\\n        merged_dfs.loc[valid_iso, 'CONTAINER_SIZE'] = '20'\\n        for prefix, size in container_size_mapping.items(): # Loop over the dictionary items and update the 'CONTAINER_SIZE' accordingly\\n            mask = merged_dfs['Size/Type'].astype(str).str.startswith(prefix)\\n            merged_dfs.loc[valid_iso & mask, 'CONTAINER_SIZE'] = size\\n        # TYPE\\n        merged_dfs['CONTAINER_TYPE'] = merged_dfs['ISO']\\n        merged_dfs.loc[valid_iso, 'CONTAINER_TYPE'] = 'GP'\\n        merged_dfs.loc[valid_iso, 'CONTAINER_TYPE'] = merged_dfs['ISO'].astype(str).str[-2:].map(ISO_Codes_for_Type)\\n        # LINE\\n        merged_dfs[\\\"LINE\\\"].ffill(inplace=True) # df[\\\"LINE\\\"].fillna(method='ffill', inplace=True)\\n        merged_dfs = merged_dfs[merged_dfs['LINE'].isin(DPWmasterCompare)]\\n    ## Adding the Necessary Information \\n    if merged_dfs.empty:\\n        error, message = True, \\\"No Data Found\\\"\\n        return pd.DataFrame(columns=['LINE', 'BL_NUMBER', 'CONTAINER_NUMBER', 'CONTAINER_SIZE', 'EMPTY_FULL', 'PORT', 'CONTAINER_TYPE', 'PROCESS']), error, message\\n    def classify_process_status(value):\\n        \\\"\\\"\\\"\\n        Splits input like 'I/F' or 'EXPORT/ECL' and returns:\\n        - PROCESS: 'I', 'E', or 'T'\\n        - EMPTY_FULL: 'FULL' or 'EMPTY'\\n        \\\"\\\"\\\"\\n        if not isinstance(value, str) or '/' not in value:\\n            return 'T', 'EMPTY'  # Default fallback\\n        parts = value.split('/')\\n        if len(parts) != 2:\\n            return 'T', 'EMPTY'\\n        # Extract PROCESS and EMPTY_FULL from the parts\\n        process_raw, ef_raw = parts[0].strip().upper(), parts[1].strip().upper()\\n        # Determine PROCESS\\n        if process_raw in ['I', 'IMPORT']:\\n            process = 'I'\\n        elif process_raw in ['E', 'EXPORT']:\\n            process = 'E'\\n        else: # process_raw in ['T', 'TRSHP']:\\n            process = 'T'\\n        # Determine EMPTY_FULL\\n        empty_full = 'FULL' if ef_raw in ['F', 'FCL'] else 'EMPTY'\\n        # RETURN\\n        return process, empty_full\\n    merged_dfs[\\\"PORT\\\"] = \\\"DPW\\\" # PORT\\n    try:\\n        merged_dfs[['PROCESS', 'EMPTY_FULL']] = merged_dfs['Status'].apply(classify_process_status).apply(pd.Series)\\n        merged_dfs = merged_dfs[[\\\"LINE\\\", \\\"BL_NUMBER\\\", \\\"CONTAINER_NUMBER\\\", \\\"CONTAINER_SIZE\\\", \\\"EMPTY_FULL\\\", \\\"PORT\\\", \\\"PROCESS\\\", \\\"CONTAINER_TYPE\\\"]]\\n    except Exception as WRONG_FORMAT_GIVEN:\\n        error, message = True, str(WRONG_FORMAT_GIVEN)\\n        return pd.DataFrame(columns=['LINE', 'BL_NUMBER', 'CONTAINER_NUMBER', 'CONTAINER_SIZE', 'EMPTY_FULL', 'PORT', 'CONTAINER_TYPE', 'PROCESS']), error, message\\n    merged_dfs = merged_dfs[merged_dfs['CONTAINER_NUMBER'].notna() & (merged_dfs['CONTAINER_NUMBER'] != '')]\\n    # NOTE: TODO- Add the ALT_PORT column!\\n    return merged_dfs, error, message\",\n",
      "      \"start_line\": 233,\n",
      "      \"end_line\": 335,\n",
      "      \"function\": \"DPW_TSR_Main\",\n",
      "      \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Inventory\\\\EXE_Extras\\\\Terminal_Main.py\",\n",
      "      \"helpers\": [\n",
      "        {\n",
      "          \"code\": \"# Extracted from Terminal_Main.py:189-232\\ndef DPW_PDF_SCRAPPER(path):\\n    import logging\\n    import pdfplumber\\n    # Kill pdfminer debug noise\\n    for name in (\\\"pdfminer\\\", \\\"pdfminer.psparser\\\", \\\"pdfminer.pdfinterp\\\"):\\n        logging.getLogger(name).setLevel(logging.ERROR)\\n    def extract_table_from_pdf(pdf_path):\\n        table = []\\n        with pdfplumber.open(pdf_path) as pdf:\\n            for page in pdf.pages:\\n                extracted = page.extract_table()\\n                if extracted:\\n                    table.extend(extracted)\\n        return table\\n    def data_ext(table):\\n        # Extract the header\\n        headers = table[0]\\n        # Extract container rows until \\\"Operator\\\" row appears\\n        container_rows = []\\n        for row in table[1:]:\\n            if row and row[0] == \\\"Operator\\\":  \\n                break\\n            container_rows.append(row)\\n        # Determine if there is only one container row\\n        singleCntr = len(container_rows) == 1\\n        # Convert container rows into a list of dictionaries\\n        extracted_data = []\\n        for row in container_rows:\\n            try:\\n                extracted_data.append({\\n                    \\\"Line\\\": row[headers.index(\\\"Line\\\")],\\n                    \\\"Container\\\": row[headers.index(\\\"Container\\\")],\\n                    \\\"ISO\\\": row[headers.index(\\\"ISO\\\")],\\n                    \\\"Size_Type\\\": row[headers.index(\\\"Sz/Type\\\")],\\n                    \\\"Status\\\": row[headers.index(\\\"Stat\\\")],\\n                    \\\"Bl No\\\": row[headers.index(\\\"BL No\\\")] if \\\"BL No\\\" in headers else \\\"\\\"\\n                })\\n            except Exception as e:\\n                print(f\\\"Error extracting row data: {e}\\\")\\n        return extracted_data, singleCntr\\n    # return data_ext(extract_table_from_pdf(path))\\n    table = extract_table_from_pdf(path)\\n    extracted_data, singleCntr = data_ext(table)\\n    return extracted_data, singleCntr\",\n",
      "          \"start_line\": 189,\n",
      "          \"end_line\": 232,\n",
      "          \"function\": \"DPW_PDF_SCRAPPER\",\n",
      "          \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Inventory\\\\EXE_Extras\\\\Terminal_Main.py\",\n",
      "          \"helpers\": []\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"code\": \"# Extracted from Terminal_Main.py:68-187\\ndef RSGT_TSR_Main(path): # , tsr_dict\\n    RSGTmasterCompare = [value for value in MasterDF[\\\"RSGT\\\"].to_list() if value != '']\\n    merged_dfs = pd.DataFrame(columns=['Line', 'Container', 'BL No', 'ISO', 'Status',\\\"PROCESS\\\"])\\n    for x in range(1,9999): \\n        try:\\n            DfSample = pd.read_excel(path, f\\\"Sheet{x}\\\") # r\\\"C:\\\\Users\\\\ACER\\\\Desktop\\\\MSS SAUDI\\\\Send Jojo\\\\Jeddah\\\\RSGT\\\\SAVE\\\\Copy of TSR.xls\\\"\\n        except ValueError as ve:\\n            print(f\\\"Error: {ve}\\\")\\n            break\\n        else:\\n            if x != 1:\\n                DfSample = DfSample.iloc[:, 2:]\\n            else:\\n                DfSample = DfSample.iloc[27:, :]\\n            DfSample.columns = DfSample.iloc[0]\\n            # Drop the first row after setting it as column headings\\n            DfSample = DfSample[1:]\\n            DfSample = DfSample.reset_index()\\n            if not (\\\"Line\\\" in DfSample.columns and \\\"Seq\\\" in DfSample.columns and \\\"Container\\\" in DfSample.columns): # TEMPORARY SOLUTION TO 2 LINES DOWN.\\n                DfSample.columns = DfSample.iloc[1]\\n                DfSample = DfSample.iloc[2:]  # Adjusted to skip the first two rows\\n            if (\\\"Line\\\" in DfSample.columns and \\\"Seq\\\" in DfSample.columns and \\\"Container\\\" in DfSample.columns) and ('Original\\\\nPosition' not in DfSample.columns and 'Departure\\\\nPosition' not in DfSample.columns):\\n                # DfSample = DfSample.iloc[:DfSample.index[DfSample['Seq'].isna()].min()-1]\\n                for index, value in enumerate(DfSample['Seq'], start=1):\\n                    if pd.notna(value) and int(value) == index:\\n                        # print(index, value)\\n                        continue\\n                    else:\\n                        # print(\\\"LAST\\\",index, value)\\n                        DfSample = DfSample.loc[:index] # [:index - 1+1]\\n                        # DfSample = DfSample.reset_index()\\n                        DfSample = DfSample.iloc[:-2] # DfSample.iloc[:len(DfSample) - 2]\\n                        # print(DfSample[['Line', 'Seq', 'Container',]])\\n                        break\\n                else:\\n                    try:\\n                        DfSample = DfSample.iloc[:DfSample[DfSample['Seq'].isnull()].index.min()-1]\\n                    except Exception as Possible_Error:\\n                        print(Possible_Error)\\n                        if 'Seq' in DfSample.columns.to_list():\\n                            null_index = DfSample[DfSample['Seq'].isnull()].index.min()\\n                            if pd.notna(null_index):\\n                                DfSample = DfSample.iloc[:null_index-1]\\n                            else:\\n                                print(\\\"Possible Error!!\\\")\\n                            #     # Handle the case when there is no null index found\\n                            #     DfSample = DfSample.iloc[:]  # Or handle in some other way\\n                        else:\\n                            print(\\\"Possible Error!?\\\")\\n                    finally:\\n                        print(\\\"Possible Error?\\\")\\n                    pass\\n                if DfSample.empty:\\n                    continue\\n                # Classification of IN & OUT can be done here.\\n                if 'POL' in DfSample.columns: # Import\\n                    # DfSample = DfSample[['Line', 'Seq', 'Container', 'BL No', 'ISO', 'Weight (Kg)', 'Status', 'Bay/Cell', 'POL', 'POD', 'Temp/Rqd', 'IMCO', 'OOG ', 'Seal No']] # ,\\\"POL\\\",\\\"Status\\\"\\n                    DfSample = DfSample[['Line', 'Container', 'BL No', 'ISO', 'Status', 'POL', 'POD']]\\n                    # print(f\\\"Sheet{x}\\\\t\\\",\\\"Import/Transshipment\\\")\\n                    DfSample[\\\"Line\\\"].ffill(inplace=True) # df[\\\"Line\\\"].fillna(method='ffill', inplace=True)\\n                    DfSample.loc[DfSample['Status'].isin([r'I/Empty', r'I/FCL']), \\\"PROCESS\\\"] = \\\"I\\\"\\n                    DfSample.loc[DfSample['Status'].isin([r'E/Empty', r'E/FCL']), \\\"PROCESS\\\"] = \\\"E\\\"\\n                    DfSample.loc[DfSample['Status'].isin([r'T/FCL',r\\\"T/Full\\\",r\\\"T/Empty\\\"]), \\\"PROCESS\\\"] = \\\"T\\\"\\n                    if 'POL' in DfSample.columns and 'POD' in DfSample.columns:\\n                        DfSample['ALT_PORT'] = DfSample['POL'].fillna(DfSample['POD']).astype(str).str.strip()\\n                    else:\\n                        if 'POL' in DfSample.columns:\\n                            DfSample['ALT_PORT'] = DfSample['POL'].astype(str).str.strip()\\n                        else:\\n                            print(\\\"POL and POD Not Found\\\")\\n                            print(DfSample.columns)\\n                            print(DfSample)\\n                            print(\\\"Please Check the Format of the File.\\\")\\n                            DfSample['ALT_PORT'] = None\\n                    merged_dfs = pd.concat([merged_dfs, DfSample], ignore_index=True)\\n                else: # Export\\n                    # DfSample = DfSample[['Line', 'Seq', 'Container', 'BL No', 'ISO', 'Weight (Kg)', 'Status', 'Bay/Cell', 'POD', 'Temp/Rqd', 'IMCO', 'OOG ', 'Seal No']] # ,\\\"POL\\\",\\\"Status\\\"\\n                    DfSample = DfSample[['Line', 'Container', 'BL No', 'ISO', 'Status', 'POD']]\\n                    # print(f\\\"Sheet{x}\\\\t\\\",\\\"Export\\\")\\n                    DfSample[\\\"Line\\\"].ffill(inplace=True) # df[\\\"Line\\\"].fillna(method='ffill', inplace=True)\\n                    DfSample.loc[DfSample['Status'].isin([r'I/Empty', r'I/FCL']), \\\"PROCESS\\\"] = \\\"I\\\"\\n                    DfSample.loc[DfSample['Status'].isin([r'E/Empty', r'E/FCL']), \\\"PROCESS\\\"] = \\\"E\\\"\\n                    DfSample.loc[DfSample['Status'].isin([r'T/FCL',r\\\"T/Full\\\",r\\\"T/Empty\\\"]), \\\"PROCESS\\\"] = \\\"T\\\"\\n                    if 'POD' in DfSample.columns:\\n                        DfSample['ALT_PORT'] = DfSample['POD'].astype(str).str.strip()\\n                    else:\\n                        DfSample['ALT_PORT'] = None\\n                    merged_dfs = pd.concat([merged_dfs, DfSample], ignore_index=True)\\n            else:\\n                # print(f\\\"Sheet{x}\\\\t\\\",False)\\n                continue\\n        # finally:\\n        #     continue\\n    ## Adding the Necessary Information \\n    if merged_dfs.empty:\\n        return pd.DataFrame(columns=['LINE', 'BL_NUMBER', 'CONTAINER_NUMBER', 'CONTAINER_SIZE', 'EMPTY_FULL', 'PORT', 'CONTAINER_TYPE', 'PROCESS'])\\n    else:\\n        merged_dfs['EMPTY_FULL'] = None\\n        merged_dfs.loc[merged_dfs['Status'] == 'I/Empty', ['EMPTY_FULL', 'PROCESS']] = ['EMPTY', 'I']\\n        merged_dfs.loc[merged_dfs['Status'].isin(['I/FCL', 'I/Full']), ['EMPTY_FULL', 'PROCESS']] = ['FULL', 'I']\\n        merged_dfs.loc[merged_dfs['Status'] == 'E/Empty', ['EMPTY_FULL', 'PROCESS']] = ['EMPTY', 'E']\\n        merged_dfs.loc[merged_dfs['Status'].isin(['E/FCL', 'E/Full']), ['EMPTY_FULL', 'PROCESS']] = ['FULL', 'E']\\n        merged_dfs.loc[merged_dfs['Status'] == 'T/Empty', ['EMPTY_FULL', 'PROCESS']] = ['EMPTY', 'T']\\n        merged_dfs.loc[merged_dfs['Status'].isin(['T/FCL', 'T/Full']), ['EMPTY_FULL', 'PROCESS']] = ['FULL', 'T']\\n        merged_dfs['CONTAINER_SIZE'] = '20' # Set a default container size for all rows\\n        for prefix, size in container_size_mapping.items(): # Loop over the dictionary items and update the 'CONTAINER_SIZE' accordingly\\n            mask = merged_dfs['ISO'].astype(str).str.startswith(prefix)\\n            merged_dfs.loc[mask, 'CONTAINER_SIZE'] = size\\n        merged_dfs['CONTAINER_TYPE'] = 'GP'\\n        merged_dfs['CONTAINER_TYPE'] = merged_dfs['ISO'].astype(str).str[-2:].map(ISO_Codes_for_Type)\\n        merged_dfs = merged_dfs[merged_dfs['Line'].isin(RSGTmasterCompare)]\\n        # merged_dfs = merged_dfs.reset_index()\\n        if merged_dfs.empty:\\n            return pd.DataFrame(columns=['LINE', 'BL_NUMBER', 'CONTAINER_NUMBER', 'CONTAINER_SIZE', 'EMPTY_FULL', 'PORT', 'CONTAINER_TYPE', 'PROCESS'])\\n        merged_dfs.rename(columns={\\\"Container\\\":\\\"CONTAINER_NUMBER\\\", \\\"BL No\\\":\\\"BL_NUMBER\\\", \\\"Line\\\":\\\"LINE\\\"}, inplace=True)\\n        merged_dfs[\\\"PORT\\\"] = \\\"RSGT\\\"\\n        if 'ALT_PORT' not in merged_dfs.columns:\\n            merged_dfs['ALT_PORT'] = None\\n        merged_dfs = merged_dfs[[\\\"LINE\\\", \\\"BL_NUMBER\\\", \\\"CONTAINER_NUMBER\\\", \\\"CONTAINER_SIZE\\\", \\\"EMPTY_FULL\\\", \\\"PORT\\\", \\\"PROCESS\\\", \\\"CONTAINER_TYPE\\\", \\\"ALT_PORT\\\"]]\\n    return merged_dfs\",\n",
      "      \"start_line\": 68,\n",
      "      \"end_line\": 187,\n",
      "      \"function\": \"RSGT_TSR_Main\",\n",
      "      \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Inventory\\\\EXE_Extras\\\\Terminal_Main.py\",\n",
      "      \"helpers\": []\n",
      "    },\n",
      "    {\n",
      "      \"code\": \"# Extracted from Terminal_Main.py:12-66\\ndef SGP_TSR_Main(path): # , tsr_dict\\n    SGPmasterCompare = [value for value in MasterDF[\\\"SGP-DMM\\\"].to_list() if value != '']\\n    merged_dfs = pd.read_excel(path)\\n    try:\\n        merged_dfs.columns = merged_dfs.iloc[12]\\n        merged_dfs = merged_dfs[13:]\\n        if \\\"Unit\\\" not in merged_dfs.columns.to_list():\\n            raise Exception(\\\"Unit Not Found\\\")\\n    except Exception as UnregisteredFormat:\\n        print(UnregisteredFormat)\\n        return pd.DataFrame(columns=['LINE', 'BL_NUMBER', 'CONTAINER_NUMBER', 'CONTAINER_SIZE', 'EMPTY_FULL', 'PORT', 'CONTAINER_TYPE', 'PROCESS'])\\n    if 'Cont. Type' in merged_dfs.columns:\\n        merged_dfs.rename(columns={'Cont. Type': 'Cont Type'}, inplace=True)\\n    if 'Status' in merged_dfs.columns:\\n        merged_dfs.rename(columns={'Status': 'Load Status'}, inplace=True)\\n    if 'Type' in merged_dfs.columns:\\n        merged_dfs.rename(columns={'Type': 'Cont Type'}, inplace=True)\\n    if 'Length' in merged_dfs.columns:\\n        merged_dfs.rename(columns={'Length': 'Unit Length'}, inplace=True)\\n    if 'BL' in merged_dfs.columns:\\n        merged_dfs.rename(columns={'BL': 'Bill of Ladding'}, inplace=True)\\n    StartingColumnList = ['SNo', 'Bill of Ladding', 'Unit', 'Unit Length', 'ISO', 'Line', 'Agent', 'Load Status', 'Move', 'FCL/LCL', 'Carrier  In', 'Voyage  In', 'POL', 'POD', 'Goods', 'IMDG1', 'UN1', 'Temp. Target', 'Gross weight', 'SEL', 'SLL'] + ['Cont Type','Carrier Out', 'Voyage Out', 'SPOD', 'Stowage Position', 'OOG']\\n    StartingColumnList = [col for col in StartingColumnList if col in merged_dfs.columns.to_list()]\\n    if \\\"POL\\\" in merged_dfs.columns and \\\"POD\\\" in merged_dfs.columns:\\n        PROCESS = \\\"I\\\"\\n        ALT_PORT_COL = \\\"POL\\\"\\n    elif \\\"POD\\\" in merged_dfs.columns and \\\"SPOD\\\" in merged_dfs.columns:\\n        PROCESS = \\\"E\\\"\\n        ALT_PORT_COL = \\\"POD\\\"\\n    else: # elif \\\"POL\\\" in merged_dfs.columns:\\n        PROCESS = \\\"T\\\"\\n        ALT_PORT_COL = \\\"POL\\\"\\n    merged_dfs = merged_dfs[StartingColumnList]\\n    merged_dfs.loc[merged_dfs['Load Status'] == 'FULL', 'EMPTY_FULL'] = 'FULL'\\n    merged_dfs.loc[merged_dfs['Load Status'] == 'EMPTY', 'EMPTY_FULL'] = 'EMPTY'\\n    merged_dfs = merged_dfs[merged_dfs['Line'].isin(SGPmasterCompare)]\\n    merged_dfs['CONTAINER_NUMBER'] = merged_dfs['Unit'].replace(re.compile(r\\\"\\\\s\\\"), '').replace(\\\" \\\",\\\"\\\")\\n    merged_dfs.rename(columns={\\\"Bill of Ladding\\\":\\\"BL_NUMBER\\\", \\\"Line\\\":\\\"LINE\\\", \\\"Unit Length\\\":\\\"CONTAINER_SIZE\\\", 'Cont Type':\\\"CONTAINER_TYPE\\\"}, inplace=True)\\n    if merged_dfs.empty:\\n        return pd.DataFrame(columns=['LINE', 'BL_NUMBER', 'CONTAINER_NUMBER', 'CONTAINER_SIZE', 'EMPTY_FULL', 'PORT', 'CONTAINER_TYPE', 'PROCESS'])\\n    merged_dfs = merged_dfs[[\\\"LINE\\\", \\\"BL_NUMBER\\\", \\\"CONTAINER_NUMBER\\\", \\\"CONTAINER_SIZE\\\", \\\"EMPTY_FULL\\\", 'Move', \\\"CONTAINER_TYPE\\\", ALT_PORT_COL]]\\n    ## Adding the Necessary Information \\n    merged_dfs[\\\"PORT\\\"] = \\\"SADMM\\\" # SGP\\n    container_type_i = merged_dfs.pop('CONTAINER_TYPE') # Remove the 'CONTAINER_TYPE' column and store it\\n    merged_dfs['CONTAINER_TYPE'] = container_type_i # Insert 'CONTAINER_TYPE' at the end of the DataFrame\\n    merged_dfs[\\\"ALT_PORT\\\"] = merged_dfs[ALT_PORT_COL].str.replace(\\\"-\\\", \\\"\\\").str.strip()\\n    merged_dfs['PROCESS'] = PROCESS\\n    # merged_dfs.loc[merged_dfs['Move'].isin([r'IMPORT']), \\\"PROCESS\\\"] = \\\"I\\\" ## Imports\\n    # merged_dfs.loc[merged_dfs['Move'].isin([r'EXPORT']), \\\"PROCESS\\\"] = \\\"E\\\" ## Exports\\n    # merged_dfs.loc[merged_dfs['Move'].isin([r'IN', r'OUT']), \\\"PROCESS\\\"] = \\\"T\\\" # merged_dfs.loc[~merged_dfs['Move'].isin([r'EXPORT', r'IMPORT']), \\\"PROCESS\\\"] = \\\"T\\\" ## Transhipment\\n    merged_dfs['DETAIL_PROCESS'] = merged_dfs['Move'].str.strip()\\n    for d in ['Move', 'index', ALT_PORT_COL]:\\n        if d in merged_dfs.columns:\\n            merged_dfs.drop(columns=[d], inplace=True)\\n    return merged_dfs\",\n",
      "      \"start_line\": 12,\n",
      "      \"end_line\": 66,\n",
      "      \"function\": \"SGP_TSR_Main\",\n",
      "      \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Inventory\\\\EXE_Extras\\\\Terminal_Main.py\",\n",
      "      \"helpers\": []\n",
      "    },\n",
      "    {\n",
      "      \"code\": \"# Extracted from jcpLogger.py:6-43\\ndef serverPrint(request, *args):\\n    if not isinstance(request, WSGIRequest):\\n        # raise TypeError(\\\"The 'request' parameter must be an instance of WSGIRequest\\\")\\n        if isinstance(request, pandas.DataFrame):\\n            request = dict(request.to_dict(orient='records')) # https://chat.openai.com/share/0e92cda9-081f-4bd0-bde9-6e79d8321cfb \\n        request = json.dumps(request)\\n        SERVER_PRINT('Marine.jcpLogger #None => \\\"Not sure (YET)\\\"', f'request PARAMETER ENTERED WRONG currently Value is :- {request}')\\n        user, module_name, http_method, url_path = None, None, None, None\\n    else:\\n        # Print the message\\n        user = request.user if request.user.is_authenticated else None\\n        module_name = request.resolver_match.func.__module__ if request.resolver_match else None\\n        http_method = request.method # GET / POST\\n        # site = request.get_host() # 127.0.0.1:8000\\n        # full_url = request.build_absolute_uri() # http://127.0.0.1:8000/imports/ATA/\\n        url_path = request.path # /imports/ATA/\\n    # Log the message with custom PRINT level\\n    converted_args = []\\n    for arg in args:\\n        arg_type = type(arg)\\n        if isinstance(arg, pandas.DataFrame):\\n            # Convert DataFrame to dictionary\\n            data_dict = arg.to_dict(orient='records') # https://chat.openai.com/share/0e92cda9-081f-4bd0-bde9-6e79d8321cfb \\n            value = json.dumps(data_dict) # Serialize the dictionary to JSON\\n        else:\\n            try:\\n                value = json.dumps(arg)\\n            except Exception as ErrorDataType:\\n                value = f\\\"ErrorDataType - {str(ErrorDataType)}\\\"\\n        converted_arg = arg_type(arg)\\n        converted_args.append(converted_arg)\\n    value = tuple(converted_args)\\n    try:\\n        SERVER_PRINT(f'{module_name} #{user} => \\\"{http_method} {url_path} \\\"', f'{value}')\\n    except Exception as e:\\n        SERVER_PRINT(f'Exception #NOTE check', f'{e}')\\n    finally:\\n        print(*args)\",\n",
      "      \"start_line\": 6,\n",
      "      \"end_line\": 43,\n",
      "      \"function\": \"serverPrint\",\n",
      "      \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Marine\\\\jcpLogger.py\",\n",
      "      \"helpers\": [\n",
      "        {\n",
      "          \"code\": \"# Extracted from settings.py:309-311\\ndef SERVER_PRINT(module_name, value):\\n    # Log the message with custom PRINT level\\n    logger.log(PRINT, f'{module_name} || {value}') # logger.log(PRINT, f'{module_name} || {value}')\",\n",
      "          \"start_line\": 309,\n",
      "          \"end_line\": 311,\n",
      "          \"function\": \"SERVER_PRINT\",\n",
      "          \"file\": \"C:\\\\Users\\\\JonathanChackoPattas\\\\OneDrive - Maritime Support Solutions\\\\Desktop\\\\MSS-Automation\\\\Marine\\\\settings.py\",\n",
      "          \"helpers\": []\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "path, func = create_file_path(BASE_PATH, Function_Path)\n",
    "print(path, \"\\n\"+func+\"()\")\n",
    "# out = extract_function_source_ast(\n",
    "#     file_path=path,\n",
    "#     func_or_qualname=func,\n",
    "#     include_helpers=True,\n",
    "#     base_path=BASE_PATH,\n",
    "#     detailed_functions=True,\n",
    "#     reursive_helper=True,\n",
    "# )\n",
    "out = extract_function_source(\n",
    "    params=ParameterInputSchema(\n",
    "        function_path=Function_Path,\n",
    "        include_helpers=True,\n",
    "        base_path=BASE_PATH,\n",
    "        detailed_functions=True,\n",
    "        recursive_helper=True,\n",
    "    )\n",
    ")\n",
    "import json\n",
    "print(json.dumps(out, indent=2))\n",
    "# for key, value in out.items():\n",
    "#     print(f\"\\n{key}:\")\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107ce54",
   "metadata": {},
   "source": [
    "# Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d75af12c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef91645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def to_function_path(base_path: str, file_path: str, func_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Reverse of create_file_path: return 'module.submodule.function' from a file path and function name.\n",
    "    - Handles package __init__.py (maps to the package name, not '...__init__').\n",
    "    - Ensures file_path is under base_path (avoids external libraries).\n",
    "    \"\"\"\n",
    "    base = Path(base_path).resolve()\n",
    "    file = Path(file_path).resolve()\n",
    "\n",
    "    # Ensure it's inside your project root\n",
    "    try:\n",
    "        rel = file.relative_to(base)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"{file} is outside BASE_PATH {base}\")\n",
    "\n",
    "    if rel.suffix != \".py\":\n",
    "        raise ValueError(\"file_path must be a .py file\")\n",
    "\n",
    "    rel_no_ext = rel.with_suffix(\"\")\n",
    "    parts = list(rel_no_ext.parts)\n",
    "\n",
    "    # If pointing at a package __init__.py, drop the final '__init__'\n",
    "    if parts and parts[-1] == \"__init__\":\n",
    "        parts = parts[:-1]\n",
    "\n",
    "    module = \".\".join(parts).strip(\".\")\n",
    "    if not module:\n",
    "        raise ValueError(\"Could not derive module name from the given path.\")\n",
    "\n",
    "    return f\"{module}.{func_name}\"\n",
    "\n",
    "Function_Path = to_function_path(BASE_PATH, path, func)\n",
    "print(Function_Path)  # Inventory.views_pack.terminal.process_exe_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ast_function_extractor.py\n",
    "from __future__ import annotations\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Tuple, List, Union\n",
    "\n",
    "FuncNode = Union[ast.FunctionDef, ast.AsyncFunctionDef]\n",
    "\n",
    "def _gather_defs(module: ast.Module) -> Tuple[Dict[str, FuncNode], Dict[str, Dict[str, FuncNode]]]:\n",
    "    \"\"\"Return (top_level_funcs, class_methods[class_name][func_name]).\"\"\"\n",
    "    top_level_funcs: Dict[str, FuncNode] = {}\n",
    "    class_methods: Dict[str, Dict[str, FuncNode]] = {}\n",
    "\n",
    "    for node in module.body:\n",
    "        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "            top_level_funcs[node.name] = node\n",
    "        elif isinstance(node, ast.ClassDef):\n",
    "            methods: Dict[str, FuncNode] = {}\n",
    "            for b in node.body:\n",
    "                if isinstance(b, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                    methods[b.name] = b\n",
    "            class_methods[node.name] = methods\n",
    "    return top_level_funcs, class_methods\n",
    "\n",
    "\n",
    "def _slice_with_decorators(src_lines: List[str], fn: FuncNode) -> Tuple[str, int, int]:\n",
    "    \"\"\"Return (code, start_line, end_line), 1-based line numbers inclusive.\"\"\"\n",
    "    start = fn.lineno\n",
    "    if getattr(fn, \"decorator_list\", None):\n",
    "        start = min(getattr(dec, \"lineno\", start) for dec in fn.decorator_list) or start\n",
    "    end = getattr(fn, \"end_lineno\", None)\n",
    "    if end is None:\n",
    "        # Fallback for very old Python: try ast.get_source_segment\n",
    "        full_src = \"\".join(src_lines)\n",
    "        seg = ast.get_source_segment(full_src, fn)\n",
    "        if seg is None:\n",
    "            raise RuntimeError(\"Unable to determine function end; please use Python 3.8+.\")\n",
    "        # Best-effort end line calc\n",
    "        end = start + seg.count(\"\\n\")\n",
    "        return seg, start, end\n",
    "    return \"\\n\".join(src_lines[start - 1 : end]), start, end\n",
    "\n",
    "\n",
    "def _called_top_level_functions(fn: FuncNode) -> List[str]:\n",
    "    \"\"\"Naive: collect ast.Name() calls used by this function.\"\"\"\n",
    "    called: set[str] = set()\n",
    "    for n in ast.walk(fn):\n",
    "        if isinstance(n, ast.Call) and isinstance(n.func, ast.Name):\n",
    "            called.add(n.func.id)\n",
    "    return sorted(called)\n",
    "\n",
    "\n",
    "def extract_function_source_ast(\n",
    "    file_path: str | Path,\n",
    "    func_or_qualname: str,\n",
    "    include_helpers: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Extract a function or method source by name.\n",
    "    - func_or_qualname: \"foo\" or \"ClassName.method\"\n",
    "    - include_helpers=True: also append any same-file top-level helper functions\n",
    "      that are directly called by the target (naive name-based detection).\n",
    "    Returns: {\"code\": str, \"start_line\": int, \"end_line\": int, \"function\": str, \"file\": str}\n",
    "    \"\"\"\n",
    "    path = Path(file_path)\n",
    "    src = path.read_text(encoding=\"utf-8\")\n",
    "    src_lines = src.splitlines()\n",
    "\n",
    "    mod = ast.parse(src)\n",
    "    top_funcs, class_methods = _gather_defs(mod)\n",
    "\n",
    "    class_name: Optional[str] = None\n",
    "    func_name = func_or_qualname\n",
    "    if \".\" in func_or_qualname:\n",
    "        class_name, func_name = func_or_qualname.split(\".\", 1)\n",
    "\n",
    "    target_node: Optional[FuncNode] = None\n",
    "    if class_name:\n",
    "        methods = class_methods.get(class_name, {})\n",
    "        target_node = methods.get(func_name)\n",
    "    else:\n",
    "        target_node = top_funcs.get(func_name)\n",
    "        # also allow class methods lookup by qualname if provided differently\n",
    "        if target_node is None:\n",
    "            for cls, methods in class_methods.items():\n",
    "                if func_name in methods:\n",
    "                    # ambiguous unless qualname given; pick first match\n",
    "                    target_node = methods[func_name]\n",
    "                    class_name = cls\n",
    "                    break\n",
    "\n",
    "    if target_node is None:\n",
    "        available = sorted(list(top_funcs.keys()) + [f\"{c}.{m}\" for c, ms in class_methods.items() for m in ms])\n",
    "        raise ValueError(f\"Function '{func_or_qualname}' not found. Available: {available}\")\n",
    "\n",
    "    main_code, start, end = _slice_with_decorators(src_lines, target_node)\n",
    "    pieces = [f\"# Extracted from {path.name}:{start}-{end}\\n{main_code}\"]\n",
    "    helper_function_paths = []\n",
    "    if include_helpers and not class_name:\n",
    "        called = _called_top_level_functions(target_node)\n",
    "        helpers = [name for name in called if name in top_funcs and name != func_name]\n",
    "        for h in helpers:\n",
    "            # h_code, hs, he = _slice_with_decorators(src_lines, top_funcs[h])\n",
    "            # pieces.append(f\"\\n# Helper '{h}' from {path.name}:{hs}-{he}\\n{h_code}\")\n",
    "            helper_function_paths.append(to_function_path(BASE_PATH, file_path, h))\n",
    "\n",
    "    return {\n",
    "        \"code\": \"\\n\".join(pieces),\n",
    "        \"start_line\": start,\n",
    "        \"end_line\": end,\n",
    "        \"function\": func_or_qualname,\n",
    "        \"file\": str(path),\n",
    "        \"helpers\": helper_function_paths,\n",
    "    }\n",
    "\n",
    "print(extract_function_source_ast(path, func, include_helpers=True)) # ['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a303c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ast_function_extractor.py\n",
    "from __future__ import annotations\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Tuple, List, Union\n",
    "\n",
    "FuncNode = Union[ast.FunctionDef, ast.AsyncFunctionDef]\n",
    "\n",
    "def _gather_defs(module: ast.Module) -> Tuple[Dict[str, FuncNode], Dict[str, Dict[str, FuncNode]]]:\n",
    "    \"\"\"Return (top_level_funcs, class_methods[class_name][func_name]).\"\"\"\n",
    "    top_level_funcs: Dict[str, FuncNode] = {}\n",
    "    class_methods: Dict[str, Dict[str, FuncNode]] = {}\n",
    "\n",
    "    for node in module.body:\n",
    "        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "            top_level_funcs[node.name] = node\n",
    "        elif isinstance(node, ast.ClassDef):\n",
    "            methods: Dict[str, FuncNode] = {}\n",
    "            for b in node.body:\n",
    "                if isinstance(b, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                    methods[b.name] = b\n",
    "            class_methods[node.name] = methods\n",
    "    return top_level_funcs, class_methods\n",
    "\n",
    "\n",
    "def _slice_with_decorators(src_lines: List[str], fn: FuncNode) -> Tuple[str, int, int]:\n",
    "    \"\"\"Return (code, start_line, end_line), 1-based line numbers inclusive.\"\"\"\n",
    "    start = fn.lineno\n",
    "    if getattr(fn, \"decorator_list\", None):\n",
    "        start = min(getattr(dec, \"lineno\", start) for dec in fn.decorator_list) or start\n",
    "    end = getattr(fn, \"end_lineno\", None)\n",
    "    if end is None:\n",
    "        # Fallback for very old Python: try ast.get_source_segment\n",
    "        full_src = \"\".join(src_lines)\n",
    "        seg = ast.get_source_segment(full_src, fn)\n",
    "        if seg is None:\n",
    "            raise RuntimeError(\"Unable to determine function end; please use Python 3.8+.\")\n",
    "        # Best-effort end line calc\n",
    "        end = start + seg.count(\"\\n\")\n",
    "        return seg, start, end\n",
    "    return \"\\n\".join(src_lines[start - 1 : end]), start, end\n",
    "\n",
    "\n",
    "def _called_top_level_functions(fn: FuncNode) -> List[str]:\n",
    "    \"\"\"Naive: collect ast.Name() calls used by this function.\"\"\"\n",
    "    called: set[str] = set()\n",
    "    for n in ast.walk(fn):\n",
    "        if isinstance(n, ast.Call) and isinstance(n.func, ast.Name):\n",
    "            called.add(n.func.id)\n",
    "    return sorted(called)\n",
    "\n",
    "\n",
    "def extract_function_source_ast(\n",
    "    file_path: str | Path,\n",
    "    func_or_qualname: str,\n",
    "    include_helpers: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Extract a function or method source by name.\n",
    "    - func_or_qualname: \"foo\" or \"ClassName.method\"\n",
    "    - include_helpers=True: also append any same-file top-level helper functions\n",
    "      that are directly called by the target (naive name-based detection).\n",
    "    Returns: {\"code\": str, \"start_line\": int, \"end_line\": int, \"function\": str, \"file\": str}\n",
    "    \"\"\"\n",
    "    path = Path(file_path)\n",
    "    src = path.read_text(encoding=\"utf-8\")\n",
    "    src_lines = src.splitlines()\n",
    "\n",
    "    mod = ast.parse(src)\n",
    "    top_funcs, class_methods = _gather_defs(mod)\n",
    "\n",
    "    class_name: Optional[str] = None\n",
    "    func_name = func_or_qualname\n",
    "    if \".\" in func_or_qualname:\n",
    "        class_name, func_name = func_or_qualname.split(\".\", 1)\n",
    "\n",
    "    target_node: Optional[FuncNode] = None\n",
    "    if class_name:\n",
    "        methods = class_methods.get(class_name, {})\n",
    "        target_node = methods.get(func_name)\n",
    "    else:\n",
    "        target_node = top_funcs.get(func_name)\n",
    "        # also allow class methods lookup by qualname if provided differently\n",
    "        if target_node is None:\n",
    "            for cls, methods in class_methods.items():\n",
    "                if func_name in methods:\n",
    "                    # ambiguous unless qualname given; pick first match\n",
    "                    target_node = methods[func_name]\n",
    "                    class_name = cls\n",
    "                    break\n",
    "\n",
    "    if target_node is None:\n",
    "        available = sorted(list(top_funcs.keys()) + [f\"{c}.{m}\" for c, ms in class_methods.items() for m in ms])\n",
    "        raise ValueError(f\"Function '{func_or_qualname}' not found. Available: {available}\")\n",
    "\n",
    "    main_code, start, end = _slice_with_decorators(src_lines, target_node)\n",
    "    pieces = [f\"# Extracted from {path.name}:{start}-{end}\\n{main_code}\"]\n",
    "\n",
    "    if include_helpers and not class_name:\n",
    "        called = _called_top_level_functions(target_node)\n",
    "        helpers = [name for name in called if name in top_funcs and name != func_name]\n",
    "        for h in helpers:\n",
    "            h_code, hs, he = _slice_with_decorators(src_lines, top_funcs[h])\n",
    "            pieces.append(f\"\\n# Helper '{h}' from {path.name}:{hs}-{he}\\n{h_code}\")\n",
    "\n",
    "    return {\n",
    "        \"code\": \"\\n\".join(pieces),\n",
    "        \"start_line\": start,\n",
    "        \"end_line\": end,\n",
    "        \"function\": func_or_qualname,\n",
    "        \"file\": str(path),\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    out = extract_function_source_ast(path, func) # , include_helpers=True\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd10394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libcst_function_extractor.py\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import libcst as cst\n",
    "from libcst import FunctionDef, ClassDef\n",
    "from libcst.metadata import MetadataWrapper, ParentNodeProvider, PositionProvider\n",
    "\n",
    "def extract_function_source_libcst(file_path: str | Path, func_or_qualname: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract the exact function/method source with original formatting preserved.\n",
    "    Supports:\n",
    "      - \"foo\" (top-level)\n",
    "      - \"ClassName.method\" (class method)\n",
    "    Returns: {\"code\": str, \"start_line\": int, \"end_line\": int, \"function\": str, \"file\": str}\n",
    "    \"\"\"\n",
    "    path = Path(file_path)\n",
    "    src = path.read_text(encoding=\"utf-8\")\n",
    "    module = cst.parse_module(src)\n",
    "    wrapper = MetadataWrapper(module)\n",
    "    pos = wrapper.resolve(PositionProvider)\n",
    "    parent = wrapper.resolve(ParentNodeProvider)\n",
    "\n",
    "    class_name = None\n",
    "    func_name = func_or_qualname\n",
    "    if \".\" in func_or_qualname:\n",
    "        class_name, func_name = func_or_qualname.split(\".\", 1)\n",
    "\n",
    "    matches: list[FunctionDef] = []\n",
    "\n",
    "    class Finder(cst.CSTVisitor):\n",
    "        METADATA_DEPENDENCIES = (ParentNodeProvider, PositionProvider)\n",
    "\n",
    "        def visit_FunctionDef(self, node: FunctionDef) -> None:\n",
    "            if node.name.value != func_name:\n",
    "                return\n",
    "            p = parent[node]\n",
    "            if class_name:\n",
    "                if isinstance(p, ClassDef) and p.name.value == class_name:\n",
    "                    matches.append(node)\n",
    "            else:\n",
    "                # top-level if parent is Module\n",
    "                from libcst import Module\n",
    "                if isinstance(p, Module):\n",
    "                    matches.append(node)\n",
    "\n",
    "    wrapper.visit(Finder())\n",
    "\n",
    "    if not matches:\n",
    "        # Build a list of available names for a helpful error\n",
    "        available: list[str] = []\n",
    "        class Collector(cst.CSTVisitor):\n",
    "            METADATA_DEPENDENCIES = (ParentNodeProvider,)\n",
    "            def visit_FunctionDef(self, node: FunctionDef) -> None:\n",
    "                p = parent[node]\n",
    "                if isinstance(p, cst.Module):\n",
    "                    available.append(node.name.value)\n",
    "                elif isinstance(p, ClassDef):\n",
    "                    available.append(f\"{p.name.value}.{node.name.value}\")\n",
    "\n",
    "        wrapper.visit(Collector())\n",
    "        raise ValueError(f\"Function '{func_or_qualname}' not found. Available: {sorted(available)}\")\n",
    "\n",
    "    node = matches[0]  # pick first match if multiple\n",
    "    code = module.code_for_node(node)\n",
    "    r = pos[node]  # CodeRange(start=(line, col), end=(line, col))\n",
    "\n",
    "    return {\n",
    "        \"code\": code,\n",
    "        \"start_line\": r.start.line,\n",
    "        \"end_line\": r.end.line,\n",
    "        \"function\": func_or_qualname,\n",
    "        \"file\": str(path),\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(extract_function_source_libcst(path, func)[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ecb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_index_extractor.py\n",
    "try:\n",
    "    from llama_index.core import SimpleDirectoryReader\n",
    "    from llama_index.core.node_parser import CodeSplitter\n",
    "except ImportError:\n",
    "    from llama_index import SimpleDirectoryReader\n",
    "    from llama_index.node_parser import CodeSplitter\n",
    "\n",
    "# Load a single Python file as a \"document\"\n",
    "docs = SimpleDirectoryReader(input_files=[Path(path)]).load_data()\n",
    "\n",
    "# Split by lines with overlap (keeps function/class blocks coherent)\n",
    "splitter = CodeSplitter(\n",
    "    language=\"python\",\n",
    "    chunk_lines=60,\n",
    "    chunk_lines_overlap=10,\n",
    "    max_chars=2000,\n",
    ")\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(docs)\n",
    "\n",
    "print(f\"Total chunks: {len(nodes)}\")\n",
    "for i, n in enumerate(nodes, 1):\n",
    "    meta = getattr(n, \"metadata\", {}) or {}\n",
    "    start = meta.get(\"start_line\") or meta.get(\"start\") or \"?\"\n",
    "    end = meta.get(\"end_line\") or meta.get(\"end\") or \"?\"\n",
    "    print(f\"\\n--- Chunk {i} [{start}-{end}] ---\")\n",
    "    print(n.text[:400])  # preview first 400 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5832592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_recursive_extractor.py\n",
    "try:\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "except ImportError:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "python_text = Path(path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "py = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "chunks = py.split_text(python_text)\n",
    "\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "for i, ch in enumerate(chunks[:5], 1):  # preview first 5\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(ch[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528bfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_python_extractor.py\n",
    "try:\n",
    "    from langchain_text_splitters import PythonCodeTextSplitter\n",
    "except ImportError:\n",
    "    from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "python_code = Path(path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "splitter = PythonCodeTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(python_code)\n",
    "\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "for i, ch in enumerate(chunks[:10], 1):  # preview first 10 small chunks\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(ch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-windows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
